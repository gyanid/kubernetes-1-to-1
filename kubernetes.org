#+Title: Learning Kubernetes
#+Date: <2018-09-22 Sat>
#+Author: Yogesh Agrawal
#+Email: yogeshiiith@gmail.com

* Introduction
  Here will be learning kubernetes.

* Concepts
  - Kubernetes improves the development, delivery and maintenance of
    distributed applications.

  - Basics: pods, labels, annotations, services and replicasets.

  - Advanced: Daemon sets, jobs, configMaps, Secrets.
 
  - Deployments: lifecycly of a complete application.

  - Storage.

  - Based on three principles: immutability, declarative
    configurationm and online self healing systems.

  - Immutability: Once an artifact is created in the system it does
    not change via user modifications.

  - There are no incremental changes.

  - Immutable container images are at the core of everything that we
    will build in Kubernetes.

  - Everything in kubernetes is a declarative configuration object
    that represents the desired state of the system. It is kubernete's
    job to ensure that the actual state of the world matches the
    desired state.

  - Imperative defines actions, declarative defines state.

  - When it receives a desired state config, it does not take actions
    to make the current state match the desired state. It continuously
    takes actions to ensure that the curent state matches the desired
    state.

  - Combining three variable growth rates into a single growth rate
    reduces statistical noise and produces a more reliable forecast of
    expected growth.

  - Efficiency can be measured by the ratio of the useful work
    performed by machine or process to the total amount of energy
    spent doing so.

  - A dockerfile can be used to automate the creation of a Docker
    container image.

  - While designing a docker file, order your layers from least likely
    to change to most likely to change in order to optimize the image
    size for pushing and pulling.

* Kubernets Components
  #+BEGIN_SRC bash
$ kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   
scheduler            Healthy   ok                   
etcd-0               Healthy   {"health": "true"}
  #+END_SRC
** controller-manager
   It is responsible for running various controllers that regulate
   behavior in the cluster: for example, ensuring that all of the
   replicas of a service are available and healthy.

** scheduler
   The scheduler is responsible for placing different pods onto
   different nodes in the cluster.

** etcd server
   It is the storage for the cluster where all of the API objects are
   stored.

* Kubernetes Nodes
   #+BEGIN_SRC bash
$ kubectl get nodes
NAME       STATUS    ROLES     AGE       VERSION
minikube   Ready     master    12d       v1.10.0
   #+END_SRC
 
   In Kubernetes nodes are separated into master nodes that containe
   containers like the API server, scheduler, etc., which manage the
   cluster, and worker nodes where your containers will
   run. Kubernetes won't generally schedule work onto master nodes to
   ensure that user workloads don't harm the overall operation of the
   cluster.

* Cluster Components
** Kubernetes Proxy
   The kubernetes proxy is responsible for routing network traffic to
   load-balanced services in the Kubernetes cluster. To do its job,
   the proxy must be present on every node in the cluster. Kubernetes
   has an API object named =DaemonSet=, which we will learn about
   later.
   #+BEGIN_SRC bash
$ kubectl get daemonSets --namespace=kube-system kube-proxy
NAME         DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
kube-proxy   1         1         1         1            1           <none>          12d
   #+END_SRC

** Kubernetes DNS
   Kubernetes also runs a DNS server, which provides naming and
   discovery for the services that are defined in the cluster. This
   DNS server also runs as a replicated service on the
   cluster. Depending on the size of your cluster, you may see one or
   more DNS servers running in your cluster. The DNS service is run as
   a Kubernetes deployment, which manages these replicas:
   #+BEGIN_SRC bash
$ kubectl get deployments --namespace=kube-system kube-dns
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-dns   1         1         1            1           12d
   #+END_SRC

   There is also a Kuberenetes service that performs load-balancing
   for the DNS server:
   #+BEGIN_SRC bash
$ kubectl get services --namespace=kube-system kube-dns
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP   12d
   #+END_SRC

   This shows that the DNS service for the cluster has the address
   =10.96.0.10=. If we log into a container in the cluster, we will
   see that the this has been polpulated into the =/etc/resolv.conf=
   file for the container.

** Kubernetes UI
   The final kubernetes components is a GUI. The UI is run as a single
   replica, but it is still managed by a Kubernetes deployment for
   reliability and upgrades.

   We can use the kubectl proxy to acces this UI.
   #+BEGIN_SRC bash
   $ kubectl proxy
   #+END_SRC

* Kubectl commands
** Namespaces
   Kubernetes uses =namespaces= to organize objects in the cluster. We
   can think of each namespace as a folder that holds a set of
   objects. By default, the kubectl command-line tool interacts with
   the default namespace. We can pass =--namespace= flag to refer a
   particular namespace.

** Contexts
   If we want to change the default namespace more permanently, we can
   use a context. This gets recorded in a kubectl configuration file,
   usually located at =$HOME/.kube/config=. This configuration file
   also stores how to both find and authenticate to your cluster. For
   example, we can create context with a different default namespace
   for your kubectl commands using:

   #+BEGIN_SRC bash
   $ kubectl config set-context my-context --namespace=mystuff
   #+END_SRC

   This creates a new context, but it doesn't actually start using it
   yet. To use this newly created context, we can run:
   #+BEGIN_SRC bash
   $ kubectl config use-context my-context
   #+END_SRC

   Contexts can also be used to manage different clusters or different
   users for authenticating to those clusters using the =--users= or
   =--clusters= flags with the =set-context= command.

** Viewing objects
   #+BEGIN_SRC bash
   $ kubectl get pods my-pod -o yaml
   $ kubectl get pods my-pod -o jsonpath --template={.status.podIP}
   $ kubectl describe <resource-name> <obj-name>
   #+END_SRC

** Creating, updating and destroying
   #+BEGIN_SRC bash
   $ kubectl apply -f obj.yaml
   $ kubectl delete -f obj.yaml
   $ kubectl delete <resource-name> <obj-name>
   #+END_SRC

** Labelling and annotating objects
   #+BEGIN_SRC bash
   $ kubectl label pods bar color=red
   $ kubectl label pods bar color-
   $ kubectl annotate pods bar color=red
   #+END_SRC

** Debugging commands
   #+BEGIN_SRC bash
   $ kubectl logs <pod-name>
   $ kubectl logs <pod-name> -f
   $ kubectl exec -it <pod-name> -- bash
   $ kubectl cp <pod-name>:/path/to/remote/file /path/to/local/file
   #+END_SRC

* Pods
  A pod represents a collection of application containers and volumes
  running in the same execution environment. Pods, not containers, are
  the smallest deployable artifact in a kubernetes cluster. This means
  all of the containers in a Pod always land on the same machine.

  Each container within a Pod runs in its own cgroup, but they share a
  number of Linux namespaces.

  Applications running in the same Pod share the same IP address and
  port space (network namespace), have the same hostname (UTS
  namespace), and can communicate using native interprocess
  communication channels over System V IPC or POSIX message queues
  (IPC namespace). However, applications in different Pods are
  isolated from each other; they have different IP addresses,
  different hostnames, and more. Containers in different Pods running
  on the same node might as well be on different servers.

** Thinking with Pods
   In general, the right question to ask yourself when designing Pods
   is, "Will these containers work correctly if they land on different
   machines? If the answer is "no", a Pod is the correct grouping for
   the containers. If the answer is "yes", multiple Pods is probably
   the correct solution.

** The Pod manifest
   Pods are described in a Pod manifest. The Pod manifest is just a
   text-file representation of the Kubernetes API object.

** Creating a Pod
   #+BEGIN_SRC bash
   $ kubectl run kuard --image=gcr.io/kuar-demo/kuard-amd64:1
   $ kubectl get pods
   $ kubectl delete deployments/kuard
   #+END_SRC

** Creating a Pod Manifest
   Pod manifests can be written using YAML or JSON, but YAML is
   generally preferred because it is slightly more human-editable and
   has the ability to add comments.

   Pod manifests include a couple of key fields and attributes: mainly
   a =metadata= section for describing the Pod and its labels, a
   =spec= section for describing volumes, and a list of containers
   that will run in the Pod.

   #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
   #+END_SRC

** Running Pods
   #+BEGIN_SRC bash
$ kubectl apply -f kuard-pod.yaml
   #+END_SRC

   The Pod manifest will be submitted to Kubernetes API server. The
   Kubernetes system will then schedule that Pod to run on a healthy
   node in the cluster, where it will be monitored by the =kubelet=
   daemon process.

** Listing Pods
   #+BEGIN_SRC bash
$ kubectl get pods
NAME                                     READY     STATUS        RESTARTS   AGE
kuard                                    1/1       Running       0          7m
   #+END_SRC

   The =Pending= state indicates that the Pod has been submitted but
   hasn't been scheduled yet.

** Pod Details
   #+BEGIN_SRC bash
$ kubectl describe pods kuard
   #+END_SRC
   
   This outputs a bunch of information about the Pod in different
   sections. At the top is basic information about the Pod.
   #+BEGIN_EXAMPLE
   Name:         kuard
Namespace:    default
Node:         minikube/192.168.64.2
Start Time:   Sun, 23 Sep 2018 10:57:17 +0100
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"kuard","namespace":"default"},"spec":{"containers":[{"image":"gcr.io/kuar-demo/kua...
Status:       Running
IP:           172.17.0.8
   #+END_EXAMPLE

   Then there is information abuot the containers running in the Pod.
   #+BEGIN_EXAMPLE
   Containers:
  kuard:
    Container ID:   docker://b141a676a5c0537cfd967fc4fb0b4e0ec7b8c8a532c103a8b58f43aeeac3ec50
    Image:          gcr.io/kuar-demo/kuard-amd64:1
    Image ID:       docker-pullable://gcr.io/kuar-demo/kuard-amd64@sha256:3e75660dfe00ba63d0e6b5db2985a7ed9c07c3e115faba291f899b05db0acd91
    Port:           8080/TCP
    State:          Running
      Started:      Sun, 23 Sep 2018 10:57:18 +0100
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7vr8q (ro)
   #+END_EXAMPLE

   Finally, there are events related to the Pod, such as when it was
   scheduled, when its image was pulled, and if/when it had to be
   restarted because of failing health checks.
   #+BEGIN_EXAMPLE
   Events:
  Type    Reason                 Age   From               Message
  ----    ------                 ----  ----               -------
  Normal  Scheduled              10m   default-scheduler  Successfully assigned kuard to minikube
  Normal  SuccessfulMountVolume  10m   kubelet, minikube  MountVolume.SetUp succeeded for volume "default-token-7vr8q"
  Normal  Pulled                 10m   kubelet, minikube  Container image "gcr.io/kuar-demo/kuard-amd64:1" already present on machine
  Normal  Created                10m   kubelet, minikube  Created container
  Normal  Started                10m   kubelet, minikube  Started container
   #+END_EXAMPLE

** Deleting a Pod
   #+BEGIN_SRC bash
$ kubectl delete pods/kuard
$ kubectl delete -f kuard-pod.yaml
   #+END_SRC

   All Pods have a termination =grace period=. By default, this is 30
   seconds. When a Pod is transitioned to =Terminating= it no longer
   receives new requests. In a serving scenario, the grace period is
   important for reliability because it allows the Pod to finish any
   active requests that it may be in the middle of processing before
   it is terminated.
   
   It's important to note that when you delete a Pod, any data stored
   in the containers associated with that Pod will be deleted as
   well. If you want to persist data across multiple instances of a
   Pod, you need to use =PersistentVolumes=.

** Accessing your Pod
*** Using Port Forwarding
    We can use the port-forwarding support built into the Kubernetes
    API and command-line tools. When we run
    #+BEGIN_SRC bash
$ kubectl port-forward kuard 8080:8080
    #+END_SRC
    a secure tunnel is created from our local machine, through the
    Kubernetes master, to the instance of the Pod running on one of
    the worker nodes.

*** Getting more info with logs
    When application needs debugging, it's helpful to be able to dig
    deeper that =describe= to understand what the application is
    doing. Kubernetes provides two commands for debugging running
    containers. The =kubectl logs= command downloads the current logs
    from the running instance:
    #+BEGIN_SRC bash
$ kubectl logs kuard
    #+END_SRC

    Adding the =-f= flag will cause you to continuously stream
    logs. The =kubectl logs= command always tries to get logs from the
    currently running container. Adding the =--previous= flag will get
    logs from a previous instance of the container.

*** Running commands in container with exec
    Sometimes logs are insufficient, and to truly determine what's
    going on you need to execute commands in the context of the
    container itself.
    #+BEGIN_SRC bash
$ kubectl exec kuard date
    #+END_SRC
    
    You can also get an interactive session by adding the =-it= flags.
    #+BEGIN_SRC bash
$ kubectl exec -it ash
    #+END_SRC

*** Copying files to and from containers
    #+BEGIN_SRC bash
$ kubectl cp <pod-name>:/captures/capture3.txt ./capture3.txt
$ kubectl cp $HOME/config.txt <pod-name>:/config.txt
    #+END_SRC

    Generally speaking, copying files into a container is an
    antipattern. We really should treat the contents of a container as
    immutable. But occasionally it's the most immediate way to stop
    the bleeding and restore your service to health, since it is
    quicker than building, pushing, and rolling out a new image. Once
    the bleeding stopped, however, it is critically important that we
    immediately go and do the image build and rollout, or we are
    guaranteed to forget the local change that we made to our
    container and overwrite it in the subsequent regularly scheduled
    rollout.

** Health Checks
   When we run application as a container in Kubernetes, it is
   automatically kept alive for you using a =process health
   check=. This health check simply ensures that the main process of
   our application is always running. If it isn't, Kubernetes restarts
   it.

   However, in some cases, a simple process check is insufficient. For
   example, if our process has deadlocked and is unable to serve
   requests, a process health check will still believe that the
   application is healthy since its process is still running.

   To address this, Kubernetes introduced health checks for
   application =liveness=. Liveness health checks run
   application-specific logic to verify that the application is not
   just still running, but is functioning properly. We have to define
   this liveness health checks in the pod manifest.

*** Liveness Probe
    Liveness probes are defined per container, which means each
    container inside a Pod is health-checked separately.

    #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      livenessProbe:
        httpGet:
          path: /healthy
          port: 8080
        initialDelaySeconds: 5
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
    #+END_SRC

    The preceding Pod manifest uses an httpGet probe to perform an
    HTTP GET request against the =/healthy= endpoint on port 8080 of
    the kuard container. The probe sets an =initialDelaySeconds= of 5,
    and thus will not be called until five seconds after all the
    containers in the Pod are created. The probe must respond within
    the one-second timeout, and the HTTP status code must be equal to
    or greater than 200 and less than 400 to be considered
    successful. Kubernetes will call the probe every 10 seconds. If
    more than three probes fail, the container will fail and restart.

*** Readiness Probe
    Kubernetes makes a distinction between =liveness= and
    =readiness=. Liveness determines if an application is running
    properly. Containers that fail liveness checks are
    restarted. Readiness describes when a container is ready to serve
    user requests. Containers that fail readiness checks are removed
    from service load balancers. Readiness probes are configured
    similarly to liveness probes.

    Combining the readiness and liveness probes helps ensure only
    health containers are running withih the cluster.

*** Types of health checks
    Kubernets also supports =tcpSocket= health checks that open a TCP
    socket; if the connection is successful, the probe succeeds. This
    style of probe is useful for non-HTTP applications; for example,
    databases or other non-HTTP-based APIs.

    Finally, Kubernetes allows =exec= probes. These executes a script
    or program in the context of the container. Following typical
    conventions, if this script returns a zero exit code, the probe
    succeeds; otherwise, it fails. =exec= scripts are often useful for
    custom application validation logic that doesn't fit nearly into
    an HTTP call.

** Resource Management
*** Resource Requests: Minimum required resources
    A pod requests the resources required to run its
    containers. Kubernetes guarantees that these resources are
    available to the Pod. The most commonly requested resources are
    CPU and memory, but Kubernetes has support for other resource
    types as well, such as GPUs and more.

    For example, to request that the kuard container lands on a
    machine with half a CPU free and gets 128 MB of memory allocated
    to it, we define the Pod as follows:
    #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
    #+END_SRC

    Note: Resources are requested oer container, not per Pod. The
    total resources requested by the Pod is the sum of all resources
    requested by all containers in the Pod. The reason for this is
    that in many cases the different containers have very different
    CPU requriements. For example, in the web server and data
    synchronizer Pod, the web server is user-facing and likely needs a
    great deal of CPU, while the data synchronizer can make do with
    very little.

    Requests are used when scheduling Pods to nodes. The Kubernetes
    scheduler will ensure that the sum of all requests of all Pods on
    a node does not exceed the capacity of the node. Therefore, a Pod
    is guaranteed to have at least the requested resources when
    running on the node. Importantly, "request" specifies a
    minimum. It does not specify a maximum cap on the resources a Pod
    may use.

    CPU requests implemented using the =cpu-shares= functionality in
    the Linux kernel.

    Memory requests are handled similarly to CPU, but there is an
    important difference. If a container is over its memory request,
    the OS can't just remove memory from the process, because it's
    been allocated. Consequently, when the system runs out of memory,
    the =kubelet= terminates containers whose memory usage is greater
    than their requested memory. These containers are automatically
    restarted, but with less available memory on the machine for the
    container to consume.

    Since resource requests guarantee resource availability to a Pod,
    they are critical to ensuring that containers have sufficient
    resources in high-load situations.

*** Capping Resource Usage with Limits
    #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
        limits:
          cpu: "1000m"
          memory: "256Mi"
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
    #+END_SRC
    
** Persisting Data with Volumes
   When a Pod is deleted or a container restarts, any and all data in
   the container's filesystem is also deleted. This is often a good
   thing, since we don't want to leave around cruft that happened to
   be written by our stateless web application.

*** Using Volumes with Pods
    To add a volume to a Pod manifest, there are two new stanzas to
    add to our configuration. The first is a new =spec.volumes=
    section. This array defines all of the volumes that may be
    accessed by containers in the Pod manifest. It's important to note
    that not all containers are required to mount all volumes defined
    in the Pod. The second addition is the =volumeMounts= array in the
    container definition. This array defines the volumes that are
    mounted into a particular container, and the path where each
    volume should be mounted. Note that two different containers in a
    Pod can mount the same volume at different mount paths.
    #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  volumes:
    - name: "kuard-data"
      hostPath:
        path: "/var/lib/kuard"
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      volumeMounts:
        - mountPath: "/data"
          name: "kuard-data"
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
    #+END_SRC

*** Different ways of using volumes with Pods
    Following are a few, and the recommended patters for Kubernetes.

**** Communication/synchronization
     To share a volume between two containers, the Pod uses an
     =emptyDir= volume. Such a volume is scoped to the Pod's lifespan,
     but it can be shared between two containers, forming the basis
     for communication between the two containers. Example our Git
     sync and web serving containers.

**** Cache
     An application may use a volume that is valuable for performance,
     but not required for correct operation of the application. For
     example, perhaps the application keeps prerendered thumbnails of
     larger images. Of course, they can be reconstructed from the
     original images, but that makes serving the thumbnails more
     expensive. You want such a cache to survive a container restart
     dut to health check failure, and thus =emptyDir= works well for
     the cache use case as well.

**** Persistent Data
     Sometimes you will use a volume for truly persistent data - data
     that is independent of the lifespan of a particular Pod, and
     should move between nodes in the cluster if a node fails or a Pod
     moves to a different machine for some reason. To achieve this,
     Kubernetes supports a wide variety of remote network storage
     volumes, including widely supported protocols like NFS or iSCSI
     as well as cloud provider network storage like AWS EBS, and
     others.

**** Mounting the host filesystem
     Other applications don't actually need a persistent volume, but
     they do need some access to the underlying host filesystem. For
     example, they may need access to the =/dev= filesystem in order
     to perform raw block-level access to a device on the system. For
     these cases, Kubernetes supports the =hostDir= volume, which can
     mount arbitrary locations on the worker node into the container.

*** Persisting Data Using Remote Disks
    Oftentimes, you want the data a Pod is using to stay with the Pod,
    even if it is restarted on a different host machine.

    To achieve this, you can mount a remote network storage volume
    into your Pod. When using network-based storage, Kubernetes
    automatically mounts and unmounts the appropriate storage whenever
    a Pod using that volume is scheduled onto a particular machine.

    There are numerous methods for mounting volumes over the
    network. Kubernetes includes support for standard protocols such
    as NFS and iSCSI as well as cloud provider-based storage APIs for
    the major cloud providers.
    #+BEGIN_SRC YAML
...
# Rest of pod definition above here
volumes:
    - name: "kuard-data"
      nfs:
        server: my.nfs.server.local
        path: "/exports"
    #+END_SRC

** Putting it All Together
   Many applications are stateful, and as such we must preserve any
   data and ensure access to the underlying storage volume regardless
   of what machine the application runs on. As we saw earlier, this
   can be achieved using a persistent volume backed by
   network-attached storage. We also want to ensure a healthy instance
   of the application is running at all times, which means we want to
   make sure the container running =kuard= is ready before we expose
   it to clients.

   Through a combination of persisten volumes, readiness and liveness
   probes, and resource restrictions Kubernetes provides everything
   needed to run stateful applications reliably.
   #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  volumes:
    - name: "kuard-data"
      nfs:
        server: my.nfs.server.local
        path: "/exports"
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
        limits:
          cpu: "1000m"
          memory: "256Mi"
      volumeMounts:
        - mountPath: "/data"
          name: "kuard-data"
      livenessProbe:
        httpGet:
          path: /healthy
          port: 8080
        initialDelaySeconds: 5
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /ready
          port: 8080
        initialDelaySeconds: 30
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3
   #+END_SRC

** Summary
   Pods represent the atomic unit of work in a Kubernestes
   cluster. Pods are comprised of one or more containers working
   together symbiotically. To create a Pod, you write a Pod manifest
   and submit it to the Kubernetes API server by using the
   command-line tool.

   Once you have submitted the manifest to the API server, the
   Kubernetes scheduler finds a machine where the Pod can fit and
   schedules the Pod to that machine. Once scheduled, the =kubelet=
   daemon on that machine is responsible for creating the containers
   that correspond to the Pod, as well as performing any health checks
   defined in the Pod manifested.

   Once a Pod is scheduled to a node, no rescheduling occurs if that
   node fails.

* Labels and Annotations
** Labels
   Labels provide identifying metadata for objects. These are
   fundamental qualities of the object that will be used for grouping,
   viewing, and operating.

   Labels have simple syntax. They are key/value pairs where both the
   key and value are represented by strings. Label keys can be broken
   down into two parts: an optional prefix and a name, separated by a
   slash. The prefix, if specified, must be shorter that 63
   characters. Names must also start and end with an alphanumeric
   character and permit the use of dashes (-), underscores (_), and
   dots (.) between characters.

   Label values are strings with a maximum length of 63
   characters. The contents of the label values follow the same rules
   as for label keys.
   |-------------------------------+-------|
   | Key                           | Value |
   |-------------------------------+-------|
   | acme.com/app-version          | 1.0.0 |
   |-------------------------------+-------|
   | appVersion                    | 1.0.0 |
   |-------------------------------+-------|
   | app.Version                   | 1.0.0 |
   |-------------------------------+-------|
   | kubernetes.io/cluster-service |  true |
   |-------------------------------+-------|

*** Applying labels
    1. Apply label to a deployment.
       #+BEGIN_SRC bash
$ kubectl label deployments alpaca-test "canary=true"
       #+END_SRC
    2. Show a label value as a column
       #+BEGIN_SRC bash
$ kubectl get deployments -L canary
       #+END_SRC
    3. Remove a label
       #+BEGIN_SRC bash
$ kubectl label deployments alpaca-test "canary-"
       #+END_SRC
    4. Select objects based on labels
       #+BEGIN_SRC bash
$ kubectl get pods --selector="ver=2,app=bandicoot"
$ kubectl get pods --selector="app in (alpaca,bandicoot)"
$ kubectl get deployments --selector="canary" # set to anything
       #+END_SRC

   Following are the selector operators
   |----------------------------+------------------------------------|
   | Operator                   | Description                        |
   |----------------------------+------------------------------------|
   | key=value                  | key is set to value                |
   |----------------------------+------------------------------------|
   | key!=value                 | key is not set to value            |
   |----------------------------+------------------------------------|
   | key in (value1, value2)    | key is one of value1 or value2     |
   |----------------------------+------------------------------------|
   | key notin (value1, value2) | key is not one of value1 or value2 |
   |----------------------------+------------------------------------|
   | key                        | key is set                         |
   |----------------------------+------------------------------------|
   | !key                       | key is not set                     |
   |----------------------------+------------------------------------|
     
** Annotations
   Annotations provide a place to store additional metadata for
   Kubernetes objects with the sole purpose of assisting tools and
   libraries.

   While labels are used to identify and group objects, annotations
   are used to provide extra information about where an object came
   from, how to use it, or policy around that object. There is
   overlap, and it is matter of taste as to when to use an annotation
   or a label. When in doubt, and information to an object as an
   annotation and promote it to a label if you find yourself wanting
   to use it in a selector.

   Annotations are used to:
   - Keep track of a "reason" for the latest update to an object.
   - Communicate a specialized scheduling policy to a specialized
     scheduler.
   - Extend data about the last tool to update the resource and how it
     was updated (used for detecting changes by other tools and doing
     a smart merge)
   - Build, release, or image information that isn't appropriate for
     labels (may include a Git hash, timestamp, PR number, etc.)
   - Enable the Deployment object to keep track of ReplicaSets that it
     is managing for rollouts.
   - Provide extra data to enhance the visual quality or usability of
     UI. For example, objects could include a link to an icon (or a
     base64-encoded version of an icon).
   - Prototype alpha functionality in Kubernetes (instead of creating
     a first-class API field, the parameters for that functionality
     are instead encoded in an annotation).

   Annotations are used in various places in Kubernetes, with the
   primary use case being rolling deployments. During rolling
   deployments, annotations are used to track rollout status and
   provide the necessary information required to roll back a
   deployment to a previous state.

*** Defining Annotations
    Annotation keys use the same format as label keys. However,
    because they are often used to communicate information between
    tools, the "namespace" part of the key is more important. Example
    keys include =deployment.kubernetes.io/revision= or
    =kubernetes.io/change-cause=.

    The value component of an annotation is a free-form string
    field. While this allows maximum flexibility as users can store
    arbitrary data, because this is arbitrary text, there is no
    validation of any format. For example, it is not uncommon for a
    JSON document to be encoded as a string and stored in an
    annotation. It is important to note that the Kubernetes server has
    no knowledge of the required format of annotation values. If
    annotations are used to pass or store data, there is no guarantee
    the data is valid. This can make tracking down errors more
    difficult.

    Annotations are defined in the common =metadata= section in every Kubernetes object:
    #+BEGIN_SRC YAML
...
metadata:
  annotations:
    example.com/icon-url: "https://example.com/icon.png"
...
    #+END_SRC

    Annotations are very convenient and provide powerful loose
    coupling. However, they should be used judiciously to avoid an
    untyped mess of data.

* Service Discovery
  While the dynamic nature of Kubernetes makes it easy to run a lot of
  things, it creates problems when it comes to finding those
  things. Most of the traditional network infrastructure wasn't build
  for the level of dynamism that Kubernetes presents.

** DNS as Service Discovery
   Service discovery tools help solve the problem of finding which
   processes are listening at which addresses for which services.

   The DNS is the traditional system of service discovery on the
   internet. DNS is designed for relatively stable name resolution
   with wide and efficient caching. It is a great system for the
   internet but falls short in the dynamic world of Kubernetes.

** The Service Object
   A =Service object= is a way to create a named label selector. Just
   as the =kubectl run= command is an easy way to create a Kubernetes
   deployment, we can use =kubectl expose= to create a service.
   #+BEGIN_SRC bash
$ kubectl run alpaca-prod --image=gcr.io/kuar-demo/kuard-amd64:1 --replicas=3 --port=8080 --labels="ver=1,app=alpaca,env=prod"
$ kubectl expose deployment alpaca-prod
$ kubectl run bandicoot-prod --image=gcr.io/kuar-demo/kuard-amd64:2 --replicas=2 --port=8080 --labels="ver=2,app=bandicoot,env=prod"
$ kubectl expose deployment bandicoot-prod
$ kubectl get services -o wide
NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR
alpaca-prod      ClusterIP   10.108.179.51   <none>        8080/TCP   6m        app=alpaca,env=prod,ver=1
bandicoot-prod   ClusterIP   10.97.117.93    <none>        8080/TCP   4m        app=bandicoot,env=prod,ver=2
kubernetes       ClusterIP   10.96.0.1       <none>        443/TCP    13d       <none>
   #+END_SRC

   The =kubernetes= service is automatically created for you so that
   you can find and talk to the Kubernetes API from within the app.

   The =kubectl expose= command conveniently pull both the label
   selector and the relevant ports (8080, in this case) from the
   deployment definition.

   Furthermore, that service is assigned a new type of virtual IP
   called a cluster IP. This is a special IP address the system will
   load-balance across all of the pods that are identified by the
   selector.
   
   To interact with services, we are going to port-forward to one of
   the alpaca pods. Start and leave this command running in a terminal
   window.
   #+BEGIN_SRC bash
$ ALPACA_PROD=$(kubectl get pods -l app=alpaca -o jsonpath='{.items[0].metadata.name}')
$ kubectl port-forward $ALPACA_PROD 48858:8080
   #+END_SRC

** Service DNS
   Because the cluster IP is virtual it is stable and it is
   appropriate to give it a DNS address. All of the issues around
   clients caching DNS results no longer apply. Within a namespace, it
   is as easy as just using the service name to connect to one of the
   pods identified by a service.

   The kubernetes DNS service provides DNS names for cluster IPs.

   The A record for =alpaca-prod= would be:
   #+BEGIN_EXAMPLE
   alpaca-prod.default.svc.cluster.local.	30	IN	A	10.108.179.51
   #+END_EXAMPLE
   Break dows is as follows:
   #+BEGIN_EXAMPLE
   alpaca-prod
       The name of the service in question.
   default
       The namespace that this service is in.
   svc
       Recognizing that this is a service. This allows Kubernetes to expose other types of things as DNS in the future.
   cluster.local
       The base domain name for the cluster. This is the default and what you will see for most clusters. 
       Administrators may change this to allow unique DNS names across multiple clusters.
   #+END_EXAMPLE

   When referring to a service in your own namespace we can just use
   the service name (alpaca-prod). We can also refer to a service in
   another namespace with =alpaca-prod.default=. We can also use the
   fully qualified service name as well.

** Readiness Checks
   One nice thing the =Service= does is track which of your pods are
   ready via a readiness check.

   Let's modify our deployment to add a readiness check:
   #+BEGIN_SRC bash
$ kubectl edit deployment/alpaca-prod
   #+END_SRC
   #+BEGIN_SRC YAML
spec:
 ...
 template:
  ...
  spec:
    containers:
      ...
      name: alpaca-prod
      readinessProbe:
        httpGet:
          path: /ready
          port: 8080
        periodSeconds: 2
        initialDelaySeconds: 0
        failureThreshold: 3
        successThreshold: 1 
   #+END_SRC

   Only ready pods are sent traffic.

   Updating the deployment definition like this will delete and
   recreate the alpaca pods. As such, we need to restart our
   port-forward command from earlier.
   #+BEGIN_SRC bash
$ ALPACA_PROD=$(kubectl get pods -l app=alpaca -o jsonpath='{.items[0].metadata.name}')
$ kubectl port-forward $ALPACA_PROD 48858:8080
   #+END_SRC

   Now open the browser to =http://localhost:48858=.
   
   In another terminal window, start a =watch= command on the
   endpoints for the =alpaca-prod= service. Endpoints are a
   lower-level way of finding what a service is sending traffic to.
   #+BEGIN_SRC bash
$ kubectl get endpoints alpaca-prod --watch  
   #+END_SRC
   The =--watch= option here causes the kubectl command to hang around
   and output any updates.

   Now go back to your browser and hit the "fail" link for the
   readiness check. After three of these this server is removed from
   the list of endpoints for the service. Hit the "Succeed" link and
   notice that after a single readiness check the endpoint is added
   back.

   This readiness check is a way for an overloaded or sick server to
   signal to the system that it doesn't want to receive traffic
   anymore. This is a great way to implement graceful shutdown. The
   server can signal that it no longer wants traffic, wait until
   existing connections are closed, and then cleanly exit.

** Looking beyond the cluster
   So far we have seen how to expose services inside a
   cluster. Oftentimes the IPs for pods are only reachable from within
   the cluster. At some point, we have to allow new traffic in!

   The most portable way to do this is to use a feature called
   =NodePorts=, which enhance a service even further. In addition to a
   cluster IP, the system picks a port (or the user can specify one),
   and every node in the cluster then forwards traffic to that port to
   the service.

   With this feature, if we can reach any node in the cluster, we can
   contact a service. We use the =NodePort= without knowing where any
   of the Pods for that service are running. This can be integrated
   with hardware or software load balancers to expose the service
   further.

   Edit alpaca-prod service:
   #+BEGIN_SRC bash
$ kubectl edit service alpaca-prod
   #+END_SRC

   Change the =spec.type= field to =NodePort=. We can also do this
   when creating the service via =kubectl expose= by specifying the
   =--type=NodePort=. This system will assign a new NodePort:
   #+BEGIN_SRC bash
$ kubectl describe service alpaca-prod
Name:                     alpaca-prod
Namespace:                default
Labels:                   app=alpaca
                          env=prod
                          ver=1
Annotations:              <none>
Selector:                 app=alpaca,env=prod,ver=1
Type:                     NodePort
IP:                       10.108.179.51
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  32362/TCP
Endpoints:                172.17.0.10:8080,172.17.0.3:8080,172.17.0.7:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
   #+END_SRC

   Here we see that the system assigned port 32362 to this
   service. Now we can hit any of our cluster nodes on that port to
   access the service. We can do ssh tunneling with something like
   this:
   #+BEGIN_SRC bash
$ ssh <node> -L 8080:localhost:32362
   #+END_SRC

   Now we can connect to that service with =http://localhost:8080=.

   Each request that we send to the service will be randomly directed
   to one of the Pods that implement the service.

** Endpoints
   Some applications want to be able to use services without using a
   cluster IP. This is done with another type of object called
   =Endpoints=. For every =Service= object, kubernetes creates a buddy
   =Endpoints= object that contains the IP addresses for that service.
   #+BEGIN_SRC bash
$ kubectl describe endpoints alpaca-prod
Name:         alpaca-prod
Namespace:    default
Labels:       app=alpaca
              env=prod
              ver=1
Annotations:  <none>
Subsets:
  Addresses:          172.17.0.10,172.17.0.3,172.17.0.7
  NotReadyAddresses:  <none>
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    <unset>  8080  TCP

Events:  <none>
   #+END_SRC

   To use a service, an advanced application can talk to the
   kubernetes API directly to look up endpoints and call them. The
   kubernetes API even has the capability to "watch" objects and be
   notified as soon as they change. In this way a client can react
   immediately as soon as the IPs associated with a service change.

   Demo:
   #+BEGIN_SRC bash
$ kubectl get endpoints alpaca-prod --watch
NAME          ENDPOINTS                                          AGE
alpaca-prod   172.17.0.10:8080,172.17.0.3:8080,172.17.0.7:8080   1h
   #+END_SRC

   Now open another terminal window and delete and recreate the
   deployment backing alpaca-prod:
   #+BEGIN_SRC bash
$ kubectl delete deployment alpaca-prod
$ kubectl run alpaca-prod --image=gcr.io/kuar-demo/kuard-amd64:1 --replicas=3 --port=8080 --labels="ver=1,app=alpaca,env=prod"
   #+END_SRC

   The output of the endpoints command will reflect that most
   up-to-date set of IP addresses associated with the service.

** Manual Service Discovery
   Kubernetes services are built on top of label selectors over
   pods. That means we can use the kubernetes API to do rudimentary
   service discovery without using a =Service= object at all!

   With kubectl (and via the API) we can easily what IPs are assigned
   to each pod in our example deployments:
   #+BEGIN_SRC bash
$ kubectl get pods -o wide --show-labels
NAME                              READY     STATUS    RESTARTS   AGE       IP           NODE       LABELS
alpaca-prod-7f94b54866-d9ddl      1/1       Running   0          55m       172.17.0.6   minikube   app=alpaca,env=prod,pod-template-hash=3950610422,ver=1
alpaca-prod-7f94b54866-fxhvq      1/1       Running   0          55m       172.17.0.7   minikube   app=alpaca,env=prod,pod-template-hash=3950610422,ver=1
alpaca-prod-7f94b54866-z62dq      1/1       Running   0          55m       172.17.0.3   minikube   app=alpaca,env=prod,pod-template-hash=3950610422,ver=1
bandicoot-prod-85ddf4c7dd-7lz7z   1/1       Running   0          2h        172.17.0.8   minikube   app=bandicoot,env=prod,pod-template-hash=4188907388,ver=2
bandicoot-prod-85ddf4c7dd-w2fgm   1/1       Running   0          2h        172.17.0.9   minikube   app=bandicoot,env=prod,pod-template-hash=4188907388,ver=2
   #+END_SRC

   This is great but what if you have a ton of pods? We'll probably
   want to filter this based on the labels applied as part of the
   deployment.
   #+BEGIN_SRC bash
$ kubectl get pods -o wide --selector=app=alpaca
NAME                           READY     STATUS    RESTARTS   AGE       IP           NODE
alpaca-prod-7f94b54866-d9ddl   1/1       Running   0          58m       172.17.0.6   minikube
alpaca-prod-7f94b54866-fxhvq   1/1       Running   0          58m       172.17.0.7   minikube
alpaca-prod-7f94b54866-z62dq   1/1       Running   0          58m       172.17.0.3   minikube
   #+END_SRC

   At this point we have the basics of service discovery. We can
   always use labels to identify the set of pods we are interested in,
   get all of the pods for those labels, and dig out the IP
   address. But keeping the correct set of labels to use in sync can
   be tricky. That is why the =Service= object was created.

** Kube-proxy and Cluster IPs
   Cluster Ips are stable virtual IPs that load-balance traffic across
   all of the endpoints in a service. This is performed by a component
   running on every node in the cluster called the =kube-proxy=.

   The =kube-proxy= watches for new services in the cluster via the
   API server. It then programs a set of =iptables= rules in the
   kernel of that host to rewrite the destination of packets so they
   are directed at one of the endpoints for that service. If the set
   of endpoints for a service changes (due to pods coming and going or
   due to a failed readiness check) the set of =iptables= rules is
   rewritten.

   The cluster IP itself is usually assigned by the API server as the
   service is created. However, when creating a service, the user can
   specify a specific cluster IP. Once set, the cluster IP cannot be
   modified without deleting and recreating the =Service= object.

   Note: The kubernetes service address range is configured using the
   =--service-cluster-ip-range= falg on the =kube-apiserver=
   binary. The service address range should not overlap with the IP
   subnets and ranges assigned to each Docker bridge or kubernetes
   node. In addition, any explicit cluster IP requested must come from
   that range and not already be in use.

** Cluster IP Environment Variables
   While most users should be using the DNS services to find cluster
   IPs, there are some older mechanisms that may still be in use. One
   of these is injecting a set of environment variables into pods as
   they start up.
   #+BEGIN_EXAMPLE
   ALPACA_PROD_SERVICE_HOST	10.108.179.51
   ALPACA_PROD_SERVICE_PORT	8080
   #+END_EXAMPLE

   A problem with the environment variable approach is that it
   requires resources to be created in a specific order. The services
   must be created before the pods that reference them. This can
   introduce quite a bit of complexity when deploying a set of
   services that make up a larger application. In addition, using just
   environment variables seems strange to many users. For this reason,
   DNS is probably the better option.

** Summary
   The =Service= object provides a flexible and powerful way to expose
   services both within the cluster and beyond. With these techniques
   we can connect services to each other and expose them outside the
   cluster.

* ReplicaSets
  Pods are one-off singletons. More often than not, we want multiple
  replicas of a container running at a particular time. Reasons for
  replication:
  - Redundancy :: Multiple running instances mean failure can be
                  tolerated.
  - Scale :: Multiple running instances mean that more requests can be
             handled.
  - Sharding :: Different replicas can handle different parts of a
                computation in parallel.

  We could manually create multiple copies of a Pod using multiple
  different (though largely similar) Pod manifests, but doing so is
  both tedious and errorprone. Logically, a user managing a replicated
  set of Pods considers them as a sinle entity to be defined and
  managed. This is precisely what a ReplicaSet is. A ReplicaSet acts
  as a cluster-wide Pod manager, ensuring that the right types and
  number of Pods are running at all times.

  Because ReplicaSets make it easy to create and manage replicated
  sets of Pods, they are the building blocks used to describe common
  application deployment patterns and provide the underpinnings
  (foundation) of self-healing for our applications at the
  infrastructure level. Pods managed by ReplicaSets are automatically
  rescheduled under certain failure conditions such as node failures
  and network partitions.

** Reconciliation Loops
   The central concepts behind a reconciliation loop is the notion of
   =desired= state and =observed= or =current= state. Desired state is
   the state we want. With a ReplicaSet it is the desired number of
   replicas and the definition of the Pod to replicate.

   The reconciliation loop is constantly running, observing the
   current state of the world and taking action to try to make the
   observed state match the desired state.

** Relating Pods and ReplicaSets
   The relationship between ReplicaSets and Pods is loosely decoupled.
   ReplicaSet create and manage Pods, they do not own the Pods they
   create. ReplicaSets use label queries to identify the set of Pods
   they should be managing. They then use the exact same Pod API that
   we use to create the Pods. Decoupling enables several important
   behaviors:

*** Adopting Existing Containers
    Early on we may be simply deploying a single pod with a container
    image without a ReplicaSet managing it. But at some point we may
    want to expand our singleton container into a replicated service
    and create and manage an array of similar containers. We may have
    even defined a load balancer that is serving traffic to that
    single pod. If ReplicaSets owned the Pods they created, then the
    only way to start replicating our pod would be to detect it and
    then relaunch it via a ReplicaSet. This might be disruptive, as
    there would be a moment in time when there would be no copies of
    our container running. However, because ReplicaSets are decoupled
    from the Pods they manage, we can simply create a ReplicaSet that
    will "adopt" the existing pod, and scale out additional copies of
    those containers. In this way we can seamlessly move from a single
    imperative pod to a replicated set of pods managed by a
    ReplicaSet.

*** Quarantining Containers
    Oftentimes, when a server misbehaves, Pod-level health checks will
    automatically restart the Pod. But if the health checks are
    incomplete, a Pod can be misbehaving but still be part of the
    replicated set. In these situations, while it would work to simply
    kill the Pod, that would leave your developers with only logs to
    debug the problem. Instead, you can modify the set of labels on
    the sick Pod. Doing so will disassociate it from the ReplicaSet
    (and service) so that you can debug the pod. The ReplicaSet
    contoller will notice that a pod is missing and create a new copy,
    but because the pod is still running, it is available to
    developers for interactive debugging, which is significantly more
    valuable than debugging from logs.

* Command cheatsheet
  #+BEGIN_SRC bash
  $ kubectl get componentstatuses
  $ kubectl get nodes
  $ kubectl describe nodes node-1
  $ kubectl get daemonSets --namespace=kube-system kube-proxy
  $ kubectl get pods --namespace=kube-system
  $ kubectl get deployments --namespace=kube-system kube-dns
  $ kubectl get services --namespace=kube-system kube-dns
  $ kubectl get deployments --namespace=kube-system kubernetes-dashboard
  $ kubectl get services --namespace=kube-system
  $ kubectl proxy
  $ kubectl get pods --no-headers
  $ kubectl get pods -o yaml
  $ kubecetl describe pods <my-pod>
  $ kubectl label pods <pod> color=red
  $ kubectl label pods <pod> label-
  $ kubectl logs <pod-name>
  $ kubectl logs <pod-name> -f
  $ kubectl exec -it <pod-name> -- bash
  $ kubectl cp <pod-name>:/path/to/remote/file /path/to/local/file
  $ kubectl help
  $ kubectl help command-name
  $ kubectl apply -f pod-manifest.yaml
  $ kubectl port-forward <pod-name> 8080:8080
  $ kubectl label deployments alpaca-test "canary=true"
  $ kubectl label deployments alpaca-test "canary=true"
  $ kubectl get pods --selector="app in (alpaca,bandicoot)"
  $ kubectl run bandicoot-staging --image=gcr.io/kuar-demo/kuard-amd64:2 --replicas=1 --labels="ver=2,app=bandicoot,env=staging"
  $ kubectl delete services,deployments -l app
  #+END_SRC

  #+BEGIN_SRC bash
  $ kubectl config current-context
  $ kubectl config use-context mas-qa/oshift-api-jfk3-qa-bamtech-co:8443/yagrawal
  $ kubectl config view
  $ kubectl -n razcp-dev get deployment razcp-hello-world-app-chart -o yaml
  $ kubectl get componentstatuses
  #+END_SRC

* References
  1. Kubernetes Up & Running book.
