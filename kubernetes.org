#+Title: Learning Kubernetes
#+Date: <2018-09-22 Sat>
#+Author: Yogesh Agrawal
#+Email: yogeshiiith@gmail.com

* Introduction
  Here will be learning kubernetes.

* Concepts
  - Kubernetes improves the development, delivery and maintenance of
    distributed applications.

  - Basics: pods, labels, annotations, services and replicasets.

  - Advanced: Daemon sets, jobs, configMaps, Secrets.
 
  - Deployments: lifecycly of a complete application.

  - Storage.

  - Based on three principles: immutability, declarative
    configurationm and online self healing systems.

  - Immutability: Once an artifact is created in the system it does
    not change via user modifications.

  - There are no incremental changes.

  - Immutable container images are at the core of everything that we
    will build in Kubernetes.

  - Everything in kubernetes is a declarative configuration object
    that represents the desired state of the system. It is kubernete's
    job to ensure that the actual state of the world matches the
    desired state.

  - Imperative defines actions, declarative defines state.

  - When it receives a desired state config, it does not take actions
    to make the current state match the desired state. It continuously
    takes actions to ensure that the curent state matches the desired
    state.

  - Combining three variable growth rates into a single growth rate
    reduces statistical noise and produces a more reliable forecast of
    expected growth.

  - Efficiency can be measured by the ratio of the useful work
    performed by machine or process to the total amount of energy
    spent doing so.

  - A dockerfile can be used to automate the creation of a Docker
    container image.

  - While designing a docker file, order your layers from least likely
    to change to most likely to change in order to optimize the image
    size for pushing and pulling.

* Kubernets Components
  #+BEGIN_SRC bash
$ kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   
scheduler            Healthy   ok                   
etcd-0               Healthy   {"health": "true"}
  #+END_SRC
** controller-manager
   It is responsible for running various controllers that regulate
   behavior in the cluster: for example, ensuring that all of the
   replicas of a service are available and healthy.

** scheduler
   The scheduler is responsible for placing different pods onto
   different nodes in the cluster.

** etcd server
   It is the storage for the cluster where all of the API objects are
   stored.

* Kubernetes Nodes
   #+BEGIN_SRC bash
$ kubectl get nodes
NAME       STATUS    ROLES     AGE       VERSION
minikube   Ready     master    12d       v1.10.0
   #+END_SRC
 
   In Kubernetes nodes are separated into master nodes that containe
   containers like the API server, scheduler, etc., which manage the
   cluster, and worker nodes where your containers will
   run. Kubernetes won't generally schedule work onto master nodes to
   ensure that user workloads don't harm the overall operation of the
   cluster.

* Cluster Components
** Kubernetes Proxy
   The kubernetes proxy is responsible for routing network traffic to
   load-balanced services in the Kubernetes cluster. To do its job,
   the proxy must be present on every node in the cluster. Kubernetes
   has an API object named =DaemonSet=, which we will learn about
   later.
   #+BEGIN_SRC bash
$ kubectl get daemonSets --namespace=kube-system kube-proxy
NAME         DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
kube-proxy   1         1         1         1            1           <none>          12d
   #+END_SRC

** Kubernetes DNS
   Kubernetes also runs a DNS server, which provides naming and
   discovery for the services that are defined in the cluster. This
   DNS server also runs as a replicated service on the
   cluster. Depending on the size of your cluster, you may see one or
   more DNS servers running in your cluster. The DNS service is run as
   a Kubernetes deployment, which manages these replicas:
   #+BEGIN_SRC bash
$ kubectl get deployments --namespace=kube-system kube-dns
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-dns   1         1         1            1           12d
   #+END_SRC

   There is also a Kuberenetes service that performs load-balancing
   for the DNS server:
   #+BEGIN_SRC bash
$ kubectl get services --namespace=kube-system kube-dns
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP   12d
   #+END_SRC

   This shows that the DNS service for the cluster has the address
   =10.96.0.10=. If we log into a container in the cluster, we will
   see that the this has been polpulated into the =/etc/resolv.conf=
   file for the container.

** Kubernetes UI
   The final kubernetes components is a GUI. The UI is run as a single
   replica, but it is still managed by a Kubernetes deployment for
   reliability and upgrades.

   We can use the kubectl proxy to acces this UI.
   #+BEGIN_SRC bash
   $ kubectl proxy
   #+END_SRC

* Kubectl commands
** Namespaces
   Kubernetes uses =namespaces= to organize objects in the cluster. We
   can think of each namespace as a folder that holds a set of
   objects. By default, the kubectl command-line tool interacts with
   the default namespace. We can pass =--namespace= flag to refer a
   particular namespace.

** Contexts
   If we want to change the default namespace more permanently, we can
   use a context. This gets recorded in a kubectl configuration file,
   usually located at =$HOME/.kube/config=. This configuration file
   also stores how to both find and authenticate to your cluster. For
   example, we can create context with a different default namespace
   for your kubectl commands using:

   #+BEGIN_SRC bash
   $ kubectl config set-context my-context --namespace=mystuff
   #+END_SRC

   This creates a new context, but it doesn't actually start using it
   yet. To use this newly created context, we can run:
   #+BEGIN_SRC bash
   $ kubectl config use-context my-context
   #+END_SRC

   Contexts can also be used to manage different clusters or different
   users for authenticating to those clusters using the =--users= or
   =--clusters= flags with the =set-context= command.

** Viewing objects
   #+BEGIN_SRC bash
   $ kubectl get pods my-pod -o yaml
   $ kubectl get pods my-pod -o jsonpath --template={.status.podIP}
   $ kubectl describe <resource-name> <obj-name>
   #+END_SRC

** Creating, updating and destroying
   #+BEGIN_SRC bash
   $ kubectl apply -f obj.yaml
   $ kubectl delete -f obj.yaml
   $ kubectl delete <resource-name> <obj-name>
   #+END_SRC

** Labelling and annotating objects
   #+BEGIN_SRC bash
   $ kubectl label pods bar color=red
   $ kubectl label pods bar color-
   $ kubectl annotate pods bar color=red
   #+END_SRC

** Debugging commands
   #+BEGIN_SRC bash
   $ kubectl logs <pod-name>
   $ kubectl logs <pod-name> -f
   $ kubectl exec -it <pod-name> -- bash
   $ kubectl cp <pod-name>:/path/to/remote/file /path/to/local/file
   #+END_SRC

* Pods
  A pod represents a collection of application containers and volumes
  running in the same execution environment. Pods, not containers, are
  the smallest deployable artifact in a kubernetes cluster. This means
  all of the containers in a Pod always land on the same machine.

  Each container within a Pod runs in its own cgroup, but they share a
  number of Linux namespaces.

  Applications running in the same Pod share the same IP address and
  port space (network namespace), have the same hostname (UTS
  namespace), and can communicate using native interprocess
  communication channels over System V IPC or POSIX message queues
  (IPC namespace). However, applications in different Pods are
  isolated from each other; they have different IP addresses,
  different hostnames, and more. Containers in different Pods running
  on the same node might as well be on different servers.

** Thinking with Pods
   In general, the right question to ask yourself when designing Pods
   is, "Will these containers work correctly if they land on different
   machines? If the answer is "no", a Pod is the correct grouping for
   the containers. If the answer is "yes", multiple Pods is probably
   the correct solution.

** The Pod manifest
   Pods are described in a Pod manifest. The Pod manifest is just a
   text-file representation of the Kubernetes API object.

** Creating a Pod
   #+BEGIN_SRC bash
   $ kubectl run kuard --image=gcr.io/kuar-demo/kuard-amd64:1
   $ kubectl get pods
   $ kubectl delete deployments/kuard
   #+END_SRC

** Creating a Pod Manifest
   Pod manifests can be written using YAML or JSON, but YAML is
   generally preferred because it is slightly more human-editable and
   has the ability to add comments.

   Pod manifests include a couple of key fields and attributes: mainly
   a =metadata= section for describing the Pod and its labels, a
   =spec= section for describing volumes, and a list of containers
   that will run in the Pod.

   #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
   #+END_SRC

** Running Pods
   #+BEGIN_SRC bash
$ kubectl apply -f kuard-pod.yaml
   #+END_SRC

   The Pod manifest will be submitted to Kubernetes API server. The
   Kubernetes system will then schedule that Pod to run on a healthy
   node in the cluster, where it will be monitored by the =kubelet=
   daemon process.

** Listing Pods
   #+BEGIN_SRC bash
$ kubectl get pods
NAME                                     READY     STATUS        RESTARTS   AGE
kuard                                    1/1       Running       0          7m
   #+END_SRC

   The =Pending= state indicates that the Pod has been submitted but
   hasn't been scheduled yet.

** Pod Details
   #+BEGIN_SRC bash
$ kubectl describe pods kuard
   #+END_SRC
   
   This outputs a bunch of information about the Pod in different
   sections. At the top is basic information about the Pod.
   #+BEGIN_EXAMPLE
   Name:         kuard
Namespace:    default
Node:         minikube/192.168.64.2
Start Time:   Sun, 23 Sep 2018 10:57:17 +0100
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"kuard","namespace":"default"},"spec":{"containers":[{"image":"gcr.io/kuar-demo/kua...
Status:       Running
IP:           172.17.0.8
   #+END_EXAMPLE

   Then there is information abuot the containers running in the Pod.
   #+BEGIN_EXAMPLE
   Containers:
  kuard:
    Container ID:   docker://b141a676a5c0537cfd967fc4fb0b4e0ec7b8c8a532c103a8b58f43aeeac3ec50
    Image:          gcr.io/kuar-demo/kuard-amd64:1
    Image ID:       docker-pullable://gcr.io/kuar-demo/kuard-amd64@sha256:3e75660dfe00ba63d0e6b5db2985a7ed9c07c3e115faba291f899b05db0acd91
    Port:           8080/TCP
    State:          Running
      Started:      Sun, 23 Sep 2018 10:57:18 +0100
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7vr8q (ro)
   #+END_EXAMPLE

   Finally, there are events related to the Pod, such as when it was
   scheduled, when its image was pulled, and if/when it had to be
   restarted because of failing health checks.
   #+BEGIN_EXAMPLE
   Events:
  Type    Reason                 Age   From               Message
  ----    ------                 ----  ----               -------
  Normal  Scheduled              10m   default-scheduler  Successfully assigned kuard to minikube
  Normal  SuccessfulMountVolume  10m   kubelet, minikube  MountVolume.SetUp succeeded for volume "default-token-7vr8q"
  Normal  Pulled                 10m   kubelet, minikube  Container image "gcr.io/kuar-demo/kuard-amd64:1" already present on machine
  Normal  Created                10m   kubelet, minikube  Created container
  Normal  Started                10m   kubelet, minikube  Started container
   #+END_EXAMPLE

** Deleting a Pod
   #+BEGIN_SRC bash
$ kubectl delete pods/kuard
$ kubectl delete -f kuard-pod.yaml
   #+END_SRC

   All Pods have a termination =grace period=. By default, this is 30
   seconds. When a Pod is transitioned to =Terminating= it no longer
   receives new requests. In a serving scenario, the grace period is
   important for reliability because it allows the Pod to finish any
   active requests that it may be in the middle of processing before
   it is terminated.
   
   It's important to note that when you delete a Pod, any data stored
   in the containers associated with that Pod will be deleted as
   well. If you want to persist data across multiple instances of a
   Pod, you need to use =PersistentVolumes=.

** Accessing your Pod
*** Using Port Forwarding
    We can use the port-forwarding support built into the Kubernetes
    API and command-line tools. When we run
    #+BEGIN_SRC bash
$ kubectl port-forward kuard 8080:8080
    #+END_SRC
    a secure tunnel is created from our local machine, through the
    Kubernetes master, to the instance of the Pod running on one of
    the worker nodes.

*** Getting more info with logs
    When application needs debugging, it's helpful to be able to dig
    deeper that =describe= to understand what the application is
    doing. Kubernetes provides two commands for debugging running
    containers. The =kubectl logs= command downloads the current logs
    from the running instance:
    #+BEGIN_SRC bash
$ kubectl logs kuard
    #+END_SRC

    Adding the =-f= flag will cause you to continuously stream
    logs. The =kubectl logs= command always tries to get logs from the
    currently running container. Adding the =--previous= flag will get
    logs from a previous instance of the container.

*** Running commands in container with exec
    Sometimes logs are insufficient, and to truly determine what's
    going on you need to execute commands in the context of the
    container itself.
    #+BEGIN_SRC bash
$ kubectl exec kuard date
    #+END_SRC
    
    You can also get an interactive session by adding the =-it= flags.
    #+BEGIN_SRC bash
$ kubectl exec -it ash
    #+END_SRC

*** Copying files to and from containers
    #+BEGIN_SRC bash
$ kubectl cp <pod-name>:/captures/capture3.txt ./capture3.txt
$ kubectl cp $HOME/config.txt <pod-name>:/config.txt
    #+END_SRC

    Generally speaking, copying files into a container is an
    antipattern. We really should treat the contents of a container as
    immutable. But occasionally it's the most immediate way to stop
    the bleeding and restore your service to health, since it is
    quicker than building, pushing, and rolling out a new image. Once
    the bleeding stopped, however, it is critically important that we
    immediately go and do the image build and rollout, or we are
    guaranteed to forget the local change that we made to our
    container and overwrite it in the subsequent regularly scheduled
    rollout.

** Health Checks
   When we run application as a container in Kubernetes, it is
   automatically kept alive for you using a =process health
   check=. This health check simply ensures that the main process of
   our application is always running. If it isn't, Kubernetes restarts
   it.

   However, in some cases, a simple process check is insufficient. For
   example, if our process has deadlocked and is unable to serve
   requests, a process health check will still believe that the
   application is healthy since its process is still running.

   To address this, Kubernetes introduced health checks for
   application =liveness=. Liveness health checks run
   application-specific logic to verify that the application is not
   just still running, but is functioning properly. We have to define
   this liveness health checks in the pod manifest.

*** Liveness Probe
    Liveness probes are defined per container, which means each
    container inside a Pod is health-checked separately.

    #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      livenessProbe:
        httpGet:
          path: /healthy
          port: 8080
        initialDelaySeconds: 5
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
    #+END_SRC

    The preceding Pod manifest uses an httpGet probe to perform an
    HTTP GET request against the =/healthy= endpoint on port 8080 of
    the kuard container. The probe sets an =initialDelaySeconds= of 5,
    and thus will not be called until five seconds after all the
    containers in the Pod are created. The probe must respond within
    the one-second timeout, and the HTTP status code must be equal to
    or greater than 200 and less than 400 to be considered
    successful. Kubernetes will call the probe every 10 seconds. If
    more than three probes fail, the container will fail and restart.

*** Readiness Probe
    Kubernetes makes a distinction between =liveness= and
    =readiness=. Liveness determines if an application is running
    properly. Containers that fail liveness checks are
    restarted. Readiness describes when a container is ready to serve
    user requests. Containers that fail readiness checks are removed
    from service load balancers. Readiness probes are configured
    similarly to liveness probes.

    Combining the readiness and liveness probes helps ensure only
    health containers are running withih the cluster.

*** Types of health checks
    Kubernets also supports =tcpSocket= health checks that open a TCP
    socket; if the connection is successful, the probe succeeds. This
    style of probe is useful for non-HTTP applications; for example,
    databases or other non-HTTP-based APIs.

    Finally, Kubernetes allows =exec= probes. These executes a script
    or program in the context of the container. Following typical
    conventions, if this script returns a zero exit code, the probe
    succeeds; otherwise, it fails. =exec= scripts are often useful for
    custom application validation logic that doesn't fit nearly into
    an HTTP call.

** Resource Management
*** Resource Requests: Minimum required resources
    A pod requests the resources required to run its
    containers. Kubernetes guarantees that these resources are
    available to the Pod. The most commonly requested resources are
    CPU and memory, but Kubernetes has support for other resource
    types as well, such as GPUs and more.

    For example, to request that the kuard container lands on a
    machine with half a CPU free and gets 128 MB of memory allocated
    to it, we define the Pod as follows:
    #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
    #+END_SRC

    Note: Resources are requested oer container, not per Pod. The
    total resources requested by the Pod is the sum of all resources
    requested by all containers in the Pod. The reason for this is
    that in many cases the different containers have very different
    CPU requriements. For example, in the web server and data
    synchronizer Pod, the web server is user-facing and likely needs a
    great deal of CPU, while the data synchronizer can make do with
    very little.

    Requests are used when scheduling Pods to nodes. The Kubernetes
    scheduler will ensure that the sum of all requests of all Pods on
    a node does not exceed the capacity of the node. Therefore, a Pod
    is guaranteed to have at least the requested resources when
    running on the node. Importantly, "request" specifies a
    minimum. It does not specify a maximum cap on the resources a Pod
    may use.

    CPU requests implemented using the =cpu-shares= functionality in
    the Linux kernel.

    Memory requests are handled similarly to CPU, but there is an
    important difference. If a container is over its memory request,
    the OS can't just remove memory from the process, because it's
    been allocated. Consequently, when the system runs out of memory,
    the =kubelet= terminates containers whose memory usage is greater
    than their requested memory. These containers are automatically
    restarted, but with less available memory on the machine for the
    container to consume.

    Since resource requests guarantee resource availability to a Pod,
    they are critical to ensuring that containers have sufficient
    resources in high-load situations.

*** Capping Resource Usage with Limits
    #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
        limits:
          cpu: "1000m"
          memory: "256Mi"
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
    #+END_SRC
    
** Persisting Data with Volumes
   When a Pod is deleted or a container restarts, any and all data in
   the container's filesystem is also deleted. This is often a good
   thing, since we don't want to leave around cruft that happened to
   be written by our stateless web application.

*** Using Volumes with Pods
    To add a volume to a Pod manifest, there are two new stanzas to
    add to our configuration. The first is a new =spec.volumes=
    section. This array defines all of the volumes that may be
    accessed by containers in the Pod manifest. It's important to note
    that not all containers are required to mount all volumes defined
    in the Pod. The second addition is the =volumeMounts= array in the
    container definition. This array defines the volumes that are
    mounted into a particular container, and the path where each
    volume should be mounted. Note that two different containers in a
    Pod can mount the same volume at different mount paths.
    #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  volumes:
    - name: "kuard-data"
      hostPath:
        path: "/var/lib/kuard"
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      volumeMounts:
        - mountPath: "/data"
          name: "kuard-data"
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
    #+END_SRC

*** Different ways of using volumes with Pods
    Following are a few, and the recommended patters for Kubernetes.

**** Communication/synchronization
     To share a volume between two containers, the Pod uses an
     =emptyDir= volume. Such a volume is scoped to the Pod's lifespan,
     but it can be shared between two containers, forming the basis
     for communication between the two containers. Example our Git
     sync and web serving containers.

**** Cache
     An application may use a volume that is valuable for performance,
     but not required for correct operation of the application. For
     example, perhaps the application keeps prerendered thumbnails of
     larger images. Of course, they can be reconstructed from the
     original images, but that makes serving the thumbnails more
     expensive. You want such a cache to survive a container restart
     dut to health check failure, and thus =emptyDir= works well for
     the cache use case as well.

**** Persistent Data
     Sometimes you will use a volume for truly persistent data - data
     that is independent of the lifespan of a particular Pod, and
     should move between nodes in the cluster if a node fails or a Pod
     moves to a different machine for some reason. To achieve this,
     Kubernetes supports a wide variety of remote network storage
     volumes, including widely supported protocols like NFS or iSCSI
     as well as cloud provider network storage like AWS EBS, and
     others.

**** Mounting the host filesystem
     Other applications don't actually need a persistent volume, but
     they do need some access to the underlying host filesystem. For
     example, they may need access to the =/dev= filesystem in order
     to perform raw block-level access to a device on the system. For
     these cases, Kubernetes supports the =hostDir= volume, which can
     mount arbitrary locations on the worker node into the container.

*** Persisting Data Using Remote Disks
    Oftentimes, you want the data a Pod is using to stay with the Pod,
    even if it is restarted on a different host machine.

    To achieve this, you can mount a remote network storage volume
    into your Pod. When using network-based storage, Kubernetes
    automatically mounts and unmounts the appropriate storage whenever
    a Pod using that volume is scheduled onto a particular machine.

    There are numerous methods for mounting volumes over the
    network. Kubernetes includes support for standard protocols such
    as NFS and iSCSI as well as cloud provider-based storage APIs for
    the major cloud providers.
    #+BEGIN_SRC YAML
...
# Rest of pod definition above here
volumes:
    - name: "kuard-data"
      nfs:
        server: my.nfs.server.local
        path: "/exports"
    #+END_SRC

** Putting it All Together
   Many applications are stateful, and as such we must preserve any
   data and ensure access to the underlying storage volume regardless
   of what machine the application runs on. As we saw earlier, this
   can be achieved using a persistent volume backed by
   network-attached storage. We also want to ensure a healthy instance
   of the application is running at all times, which means we want to
   make sure the container running =kuard= is ready before we expose
   it to clients.

   Through a combination of persisten volumes, readiness and liveness
   probes, and resource restrictions Kubernetes provides everything
   needed to run stateful applications reliably.
   #+BEGIN_SRC YAML
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  volumes:
    - name: "kuard-data"
      nfs:
        server: my.nfs.server.local
        path: "/exports"
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:1
      name: kuard
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
        limits:
          cpu: "1000m"
          memory: "256Mi"
      volumeMounts:
        - mountPath: "/data"
          name: "kuard-data"
      livenessProbe:
        httpGet:
          path: /healthy
          port: 8080
        initialDelaySeconds: 5
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /ready
          port: 8080
        initialDelaySeconds: 30
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3
   #+END_SRC

** Summary
   Pods represent the atomic unit of work in a Kubernestes
   cluster. Pods are comprised of one or more containers working
   together symbiotically. To create a Pod, you write a Pod manifest
   and submit it to the Kubernetes API server by using the
   command-line tool.

   Once you have submitted the manifest to the API server, the
   Kubernetes scheduler finds a machine where the Pod can fit and
   schedules the Pod to that machine. Once scheduled, the =kubelet=
   daemon on that machine is responsible for creating the containers
   that correspond to the Pod, as well as performing any health checks
   defined in the Pod manifested.

   Once a Pod is scheduled to a node, no rescheduling occurs if that
   node fails.

* Command cheatsheet
  #+BEGIN_SRC bash
  $ kubectl get componentstatuses
  $ kubectl get nodes
  $ kubectl describe nodes node-1
  $ kubectl get daemonSets --namespace=kube-system kube-proxy
  $ kubectl get pods --namespace=kube-system
  $ kubectl get deployments --namespace=kube-system kube-dns
  $ kubectl get services --namespace=kube-system kube-dns
  $ kubectl get deployments --namespace=kube-system kubernetes-dashboard
  $ kubectl get services --namespace=kube-system
  $ kubectl proxy
  $ kubectl get pods --no-headers
  $ kubectl get pods -o yaml
  $ kubecetl describe pods <my-pod>
  $ kubectl label pods <pod> color=red
  $ kubectl label pods <pod> label-
  $ kubectl logs <pod-name>
  $ kubectl logs <pod-name> -f
  $ kubectl exec -it <pod-name> -- bash
  $ kubectl cp <pod-name>:/path/to/remote/file /path/to/local/file
  $ kubectl help
  $ kubectl help command-name
  $ kubectl apply -f pod-manifest.yaml
  $ kubectl port-forward <pod-name> 8080:8080
  #+END_SRC

  #+BEGIN_SRC bash
  $ kubectl config current-context
  $ kubectl config use-context mas-qa/oshift-api-jfk3-qa-bamtech-co:8443/yagrawal
  $ kubectl config view
  $ kubectl -n razcp-dev get deployment razcp-hello-world-app-chart -o yaml
  $ kubectl get componentstatuses
  #+END_SRC

* References
  1. Kubernetes Up & Running book.
